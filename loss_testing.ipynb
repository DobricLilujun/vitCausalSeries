{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultiScaleLoss(torch.nn.Module):\n",
    "    def __init__(self, scales=[1, 0.5, 0.25], weight_mse=1.0, weight_sam=1.0):\n",
    "        \"\"\"\n",
    "        Multi-Scale Loss combining MSE and SAM.\n",
    "\n",
    "        Args:\n",
    "            scales (list): Downscaling factors for multi-scale loss.\n",
    "            weight_mse (float): Weight for the MSE loss component.\n",
    "            weight_sam (float): Weight for the SAM loss component.\n",
    "        \"\"\"\n",
    "        super(MultiScaleLoss, self).__init__()\n",
    "        self.scales = scales\n",
    "        self.weight_mse = weight_mse\n",
    "        self.weight_sam = weight_sam\n",
    "\n",
    "    def forward(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the combined loss.\n",
    "\n",
    "        Args:\n",
    "            y_true (torch.Tensor): Ground truth tensor of shape (N, C, H, W).\n",
    "            y_pred (torch.Tensor): Predicted tensor of shape (N, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The combined loss.\n",
    "        \"\"\"\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for scale in self.scales:\n",
    "            if scale < 1:\n",
    "                y_true_scaled = F.interpolate(\n",
    "                    y_true, scale_factor=scale, mode=\"bilinear\", align_corners=False\n",
    "                )\n",
    "                y_pred_scaled = F.interpolate(\n",
    "                    y_pred, scale_factor=scale, mode=\"bilinear\", align_corners=False\n",
    "                )\n",
    "            else:\n",
    "                y_true_scaled = y_true\n",
    "                y_pred_scaled = y_pred\n",
    "\n",
    "            # MSE Loss\n",
    "            mse_loss = F.mse_loss(y_pred_scaled, y_true_scaled)\n",
    "\n",
    "            # SAM Loss\n",
    "            sam_loss = self.sam_loss(y_true_scaled, y_pred_scaled)\n",
    "\n",
    "            # Combine losses\n",
    "            total_loss += self.weight_mse * mse_loss + self.weight_sam * sam_loss\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    @staticmethod\n",
    "    def sam_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Spectral Angle Mapper (SAM) loss.\n",
    "\n",
    "        Args:\n",
    "            y_true (torch.Tensor): Ground truth tensor of shape (N, C, H, W).\n",
    "            y_pred (torch.Tensor): Predicted tensor of shape (N, C, H, W).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The SAM loss.\n",
    "        \"\"\"\n",
    "        # Flatten spatial dimensions for vectorized computation\n",
    "        y_true_flat = y_true.view(y_true.size(0), y_true.size(1), -1)  # (N, C, H*W)\n",
    "        y_pred_flat = y_pred.view(y_pred.size(0), y_pred.size(1), -1)  # (N, C, H*W)\n",
    "\n",
    "        # Compute dot product and norms\n",
    "        dot_product = torch.sum(y_true_flat * y_pred_flat, dim=1)  # (N, H*W)\n",
    "        norm_true = torch.norm(y_true_flat, dim=1) + 1e-8  # Avoid division by zero\n",
    "        norm_pred = torch.norm(y_pred_flat, dim=1) + 1e-8\n",
    "\n",
    "        # Compute SAM\n",
    "        sam = torch.acos(\n",
    "            torch.clamp(dot_product / (norm_true * norm_pred), -1.0, 1.0)\n",
    "        )  # (N, H*W)\n",
    "        return torch.mean(sam)  # Mean over all pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'loss_fn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m y_true \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m))  \u001b[38;5;66;03m# Example ground truth with 10 channels\u001b[39;00m\n\u001b[1;32m      2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand((\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m))  \u001b[38;5;66;03m# Example prediction with 10 channels\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_fn\u001b[49m(y_true, y_pred)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss_fn' is not defined"
     ]
    }
   ],
   "source": [
    "y_true = torch.rand((4, 10, 128, 128))  # Example ground truth with 10 channels\n",
    "y_pred = torch.rand((4, 10, 128, 128))  # Example prediction with 10 channels\n",
    "\n",
    "loss_fn = MultiScaleLoss(scales=[1, 0.5, 0.25], weight_mse=1.0, weight_sam=1.0)\n",
    "\n",
    "loss = loss_fn(y_true, y_pred)\n",
    "print(\"Loss:\", loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始张量:\n",
      " tensor([[[[ 0,  1],\n",
      "          [ 2,  3]],\n",
      "\n",
      "         [[ 4,  5],\n",
      "          [ 6,  7]],\n",
      "\n",
      "         [[ 8,  9],\n",
      "          [10, 11]],\n",
      "\n",
      "         [[12, 13],\n",
      "          [14, 15]],\n",
      "\n",
      "         [[16, 17],\n",
      "          [18, 19]],\n",
      "\n",
      "         [[20, 21],\n",
      "          [22, 23]],\n",
      "\n",
      "         [[24, 25],\n",
      "          [26, 27]],\n",
      "\n",
      "         [[28, 29],\n",
      "          [30, 31]],\n",
      "\n",
      "         [[32, 33],\n",
      "          [34, 35]],\n",
      "\n",
      "         [[36, 37],\n",
      "          [38, 39]],\n",
      "\n",
      "         [[40, 41],\n",
      "          [42, 43]],\n",
      "\n",
      "         [[44, 45],\n",
      "          [46, 47]],\n",
      "\n",
      "         [[48, 49],\n",
      "          [50, 51]],\n",
      "\n",
      "         [[52, 53],\n",
      "          [54, 55]],\n",
      "\n",
      "         [[56, 57],\n",
      "          [58, 59]],\n",
      "\n",
      "         [[60, 61],\n",
      "          [62, 63]],\n",
      "\n",
      "         [[64, 65],\n",
      "          [66, 67]],\n",
      "\n",
      "         [[68, 69],\n",
      "          [70, 71]]]])\n",
      "\n",
      "切割后的张量形状: torch.Size([1, 3, 6, 2, 2])\n",
      "切割后的张量:\n",
      " tensor([[[[[ 0,  1],\n",
      "           [ 2,  3]],\n",
      "\n",
      "          [[ 4,  5],\n",
      "           [ 6,  7]],\n",
      "\n",
      "          [[ 8,  9],\n",
      "           [10, 11]],\n",
      "\n",
      "          [[12, 13],\n",
      "           [14, 15]],\n",
      "\n",
      "          [[16, 17],\n",
      "           [18, 19]],\n",
      "\n",
      "          [[20, 21],\n",
      "           [22, 23]]],\n",
      "\n",
      "\n",
      "         [[[24, 25],\n",
      "           [26, 27]],\n",
      "\n",
      "          [[28, 29],\n",
      "           [30, 31]],\n",
      "\n",
      "          [[32, 33],\n",
      "           [34, 35]],\n",
      "\n",
      "          [[36, 37],\n",
      "           [38, 39]],\n",
      "\n",
      "          [[40, 41],\n",
      "           [42, 43]],\n",
      "\n",
      "          [[44, 45],\n",
      "           [46, 47]]],\n",
      "\n",
      "\n",
      "         [[[48, 49],\n",
      "           [50, 51]],\n",
      "\n",
      "          [[52, 53],\n",
      "           [54, 55]],\n",
      "\n",
      "          [[56, 57],\n",
      "           [58, 59]],\n",
      "\n",
      "          [[60, 61],\n",
      "           [62, 63]],\n",
      "\n",
      "          [[64, 65],\n",
      "           [66, 67]],\n",
      "\n",
      "          [[68, 69],\n",
      "           [70, 71]]]]])\n",
      "\n",
      "时间步 0: 通道数据\n",
      "tensor([[[ 0,  1],\n",
      "         [ 2,  3]],\n",
      "\n",
      "        [[24, 25],\n",
      "         [26, 27]],\n",
      "\n",
      "        [[48, 49],\n",
      "         [50, 51]]])\n",
      "\n",
      "时间步 1: 通道数据\n",
      "tensor([[[ 4,  5],\n",
      "         [ 6,  7]],\n",
      "\n",
      "        [[28, 29],\n",
      "         [30, 31]],\n",
      "\n",
      "        [[52, 53],\n",
      "         [54, 55]]])\n",
      "\n",
      "时间步 2: 通道数据\n",
      "tensor([[[ 8,  9],\n",
      "         [10, 11]],\n",
      "\n",
      "        [[32, 33],\n",
      "         [34, 35]],\n",
      "\n",
      "        [[56, 57],\n",
      "         [58, 59]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 输入张量 (B=1, C=10, H=2, W=2)\n",
    "x = torch.arange(72).view(1, 18, 2, 2)  # 数据从 0 到 31\n",
    "print(\"原始张量:\\n\", x)\n",
    "\n",
    "# 参数设置\n",
    "time_span = 3\n",
    "conv_input_channel = 18 // time_span  # 每个时间步的通道数\n",
    "\n",
    "# 通过 view 调整形状\n",
    "test = x.view(1, time_span, conv_input_channel, 2, 2)\n",
    "\n",
    "# 原始与切割后的结果比较\n",
    "print(\"\\n切割后的张量形状:\", test.shape)\n",
    "print(\"切割后的张量:\\n\", test)\n",
    "\n",
    "# 验证切割后每个时间步的内容\n",
    "for t in range(time_span):\n",
    "    print(f\"\\n时间步 {t}: 通道数据\")\n",
    "    print(test[0, :, t, :, :])  # 提取第 t 个时间步的通道数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0,  1],\n",
       "          [ 2,  3]],\n",
       "\n",
       "         [[ 4,  5],\n",
       "          [ 6,  7]],\n",
       "\n",
       "         [[ 8,  9],\n",
       "          [10, 11]],\n",
       "\n",
       "         [[12, 13],\n",
       "          [14, 15]],\n",
       "\n",
       "         [[16, 17],\n",
       "          [18, 19]],\n",
       "\n",
       "         [[20, 21],\n",
       "          [22, 23]]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[:, 0, :, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 8,  9],\n",
       "         [10, 11]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test.view(1, time_span * conv_input_channel, 2, 2)\n",
    "test[:, 2, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_vit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
