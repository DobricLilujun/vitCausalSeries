{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个主要做baseline，也就是给一对时间序列的图片，然后让模型自己生成目标的恢复之后的结果。\n",
    "\n",
    "\n",
    "这个既不用时间序列，也不用参考照片，只用目标序列前后的图片\n",
    "\n",
    "\n",
    "然后云用mask做掩码来掩盖掉训练时的损失。\n",
    "\n",
    "这个可以做baseline，因为他非常简单，并且比较容易实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/snt/miniconda3/envs/causal_vit_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/snt/miniconda3/envs/causal_vit_env/lib/python3.10/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/home/snt/miniconda3/envs/causal_vit_env/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* STARTED AT ************ 2025-01-16 23:58:31.373983\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Importing required libraries\n",
    "# --------------------------------------------------------------------\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL.Image as Image\n",
    "from glob import glob\n",
    "from mymodels import VisionTransformer, ReconNet, Classifier\n",
    "from mymodels.unet import Unet\n",
    "from mymodels.discriminatorv2 import Discriminator\n",
    "import matplotlib.pyplot as plt\n",
    "from myutils import imshow\n",
    "from datetime import datetime\n",
    "\n",
    "# importing pytorch functions\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as tvtransforms\n",
    "from torch.nn import SmoothL1Loss, BCELoss\n",
    "\n",
    "# importing utils required in th code\n",
    "from utils import subsample\n",
    "from utils import transforms\n",
    "from utils.evaluate import ssim, psnr, nmse\n",
    "from utils.losses import (\n",
    "    SSIMLoss,\n",
    "    SSIMLoss_V1,\n",
    "    MultiScaleLoss,\n",
    "    spectral_angle_mapper_numpy,\n",
    "    peak_signal_to_noise_ratio,\n",
    "    structural_similarity_index,\n",
    "    CrossEntropyWithNaNMask,\n",
    ")\n",
    "from utils.process import transform_tensor_image, RealisticCloudMaskFunc\n",
    "from osgeo import gdal\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Device\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = 'cuda'\n",
    "# np.random.seed(42)\n",
    "# random.seed(42)\n",
    "\n",
    "print(\"******* STARTED AT ************\", datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "def calculate_accuracy_excluding_label(all_targets, all_preds, label_to_remove):\n",
    "    flat_targets = all_targets.flatten()\n",
    "    flat_preds = all_preds.flatten()\n",
    "\n",
    "    filtered_targets = flat_targets[flat_targets != label_to_remove]\n",
    "    filtered_preds = flat_preds[flat_targets != label_to_remove]\n",
    "    overall_accuracy = accuracy_score(filtered_targets, filtered_preds)\n",
    "\n",
    "    return overall_accuracy\n",
    "\n",
    "\n",
    "def calculate_f1_score_excluding_label(all_targets, all_preds, label_to_remove):\n",
    "    flat_targets = all_targets.flatten()\n",
    "    flat_preds = all_preds.flatten()\n",
    "\n",
    "    filtered_targets = flat_targets[flat_targets != label_to_remove]\n",
    "    filtered_preds = flat_preds[flat_targets != label_to_remove]\n",
    "    f1 = f1_score(filtered_targets, filtered_preds, average=\"weighted\")\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenetDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        isval=False,\n",
    "        add_cloud_mask=True,\n",
    "        add_kspace_mask=False,\n",
    "        val_path=\"/home/snt/projects_lujun/Image-Reconstruction-by-Vision-Transformer/output_all_times/val\",\n",
    "        train_path=\"/home/snt/projects_lujun/Image-Reconstruction-by-Vision-Transformer/output_all_times/train\",\n",
    "    ):\n",
    "\n",
    "        self.add_cloud_mask = add_cloud_mask\n",
    "        self.add_kspace_mask = add_kspace_mask\n",
    "        self.val_path = val_path\n",
    "        self.train_path = train_path\n",
    "        if isval:\n",
    "            ## combine paths of each imagenet validation image into a single list\n",
    "            self.files = []\n",
    "            pattern = \"*.tif\"\n",
    "            for dir, _, _ in os.walk(self.val_path):\n",
    "                self.files.extend(glob(os.path.join(dir, pattern)))\n",
    "        else:\n",
    "            ## combine paths of each imagenet training image into a single list\n",
    "            self.files = []  # get path of each imagenet images\n",
    "            pattern = \"*.tif\"\n",
    "            for dir, _, _ in os.walk(self.train_path):\n",
    "                self.files.extend(glob(os.path.join(dir, pattern)))\n",
    "\n",
    "        self.transform = transform_tensor_image\n",
    "        self.factors = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    def read_tiff(self, file_path):\n",
    "        dataset = gdal.Open(file_path)\n",
    "        if not dataset:\n",
    "            raise FileNotFoundError(f\"Unable to open the file: {file_path}\")\n",
    "        band_count = dataset.RasterCount\n",
    "        bands_data = []\n",
    "        for i in range(1, band_count + 1):\n",
    "            band = dataset.GetRasterBand(i)\n",
    "            band_name = band.GetDescription()\n",
    "            band_data = band.ReadAsArray()\n",
    "            bands_data.append(band_data)\n",
    "\n",
    "        image = np.stack(bands_data, axis=0)\n",
    "        return image\n",
    "\n",
    "    def get_mask_func(\n",
    "        self,\n",
    "        samp_style,\n",
    "        factor,\n",
    "    ):\n",
    "        center_fractions = 0.08 * 4 / factor\n",
    "        if samp_style == \"random\":\n",
    "            mask_func = subsample.RandomMaskFunc(\n",
    "                center_fractions=[center_fractions],\n",
    "                accelerations=[factor],\n",
    "            )\n",
    "        elif samp_style == \"equidist\":\n",
    "            mask_func = subsample.EquispacedMaskFunc(\n",
    "                center_fractions=[center_fractions],\n",
    "                accelerations=[factor],\n",
    "            )\n",
    "        return mask_func\n",
    "\n",
    "    def add_gaussian_noise(self, x):\n",
    "        ch, row, col = x.shape\n",
    "        mean = 0\n",
    "        var = 0.05\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean, sigma, (ch, row, col))\n",
    "        gauss = gauss.reshape(ch, row, col)\n",
    "        noisy = x + gauss\n",
    "        return noisy.float()\n",
    "\n",
    "    def __len__(\n",
    "        self,\n",
    "    ):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        channel_count = 11\n",
    "        tiff = torch.from_numpy(self.read_tiff(self.files[idx]))\n",
    "        file_path = self.files[idx]\n",
    "        file_name = os.path.basename(file_path)\n",
    "        # tiff = self.transform(tiff, target_size=64 * 3, crop_size=60 * 3)\n",
    "        channels, height, width = tiff.shape\n",
    "        time_span = int((channels - 1) / channel_count)\n",
    "        image = tiff[0:channel_count, :, :]\n",
    "\n",
    "        y = image\n",
    "        cloud_mask = torch.isnan(y).any(dim=0)\n",
    "        cloud_mask = (~cloud_mask).int()\n",
    "        y[:, cloud_mask == 0] = 0\n",
    "        original_spaces_transformed = y.clone()\n",
    "        masked_kspace = y.clone()\n",
    "\n",
    "        if random.uniform(0, 1) < 0.5:\n",
    "            y = torch.rot90(y, 1, [-2, -1])\n",
    "\n",
    "        if random.uniform(0, 1) < 0.5:\n",
    "            samp_style = \"random\"\n",
    "        else:\n",
    "            samp_style = \"equidist\"\n",
    "\n",
    "        # factor = random.choice(self.factors)\n",
    "        mask_func = self.get_mask_func(samp_style, factor=2.0)\n",
    "\n",
    "        if self.add_kspace_mask:  # add kspace mask\n",
    "            masked_kspace, _ = transforms.apply_mask(masked_kspace, mask_func)\n",
    "        if self.add_cloud_mask:  # add cloud mask\n",
    "            masked_kspace, added_cloud_mask = RealisticCloudMaskFunc(masked_kspace)\n",
    "            added_cloud_mask = added_cloud_mask.unsqueeze(0)\n",
    "        else:\n",
    "            added_cloud_mask = torch.ones_like(cloud_mask.unsqueeze(0))\n",
    "\n",
    "        cls_targets = np.squeeze(tiff[-1])\n",
    "\n",
    "        unique_classes = torch.unique(cls_targets[~torch.isnan(cls_targets)])\n",
    "        num_class = len(unique_classes)\n",
    "        class_mapping = {value.item(): idx for idx, value in enumerate(unique_classes)}\n",
    "        cls_targets = torch.nan_to_num(cls_targets, nan=num_class)\n",
    "        cls_targets = cls_targets.apply_(lambda x: class_mapping.get(x, num_class))\n",
    "        cls_targets = cls_targets.long()\n",
    "\n",
    "        return (\n",
    "            masked_kspace,  # inputs\n",
    "            original_spaces_transformed,  # tragets\n",
    "            cloud_mask,\n",
    "            added_cloud_mask,\n",
    "            cls_targets,\n",
    "            file_name,\n",
    "        )\n",
    "\n",
    "\n",
    "train_dataset = ImagenetDataset(\n",
    "    isval=False,\n",
    "    add_cloud_mask=False,\n",
    "    add_kspace_mask=False,\n",
    ")\n",
    "\n",
    "\n",
    "# ntrain = 1000\n",
    "# train_dataset, _ = torch.utils.data.random_split(\n",
    "#     train_dataset,\n",
    "#     [ntrain, len(train_dataset) - ntrain],\n",
    "#     generator=torch.Generator().manual_seed(42),\n",
    "# )\n",
    "\n",
    "val_dataset = ImagenetDataset(\n",
    "    isval=True,\n",
    "    add_cloud_mask=False,\n",
    "    add_kspace_mask=False,\n",
    ")\n",
    "\n",
    "batch_size = 15\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    # num_workers=2,\n",
    "    pin_memory=True,\n",
    "    generator=torch.Generator().manual_seed(seed),\n",
    ")\n",
    "valloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    # num_workers=1,\n",
    "    pin_memory=True,\n",
    "    generator=torch.Generator().manual_seed(seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the mask and future application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def read_tiff(file_path):\n",
    "    dataset = gdal.Open(file_path)\n",
    "    if not dataset:\n",
    "        raise FileNotFoundError(f\"Unable to open the file: {file_path}\")\n",
    "    band_count = dataset.RasterCount\n",
    "    bands_data = []\n",
    "    for i in range(1, band_count + 1):\n",
    "        band = dataset.GetRasterBand(i)\n",
    "        band_name = band.GetDescription()\n",
    "        band_data = band.ReadAsArray()\n",
    "        bands_data.append(band_data)\n",
    "\n",
    "    image = np.stack(bands_data, axis=0)\n",
    "    return image\n",
    "\n",
    "\n",
    "def create_time_series_pseudo_color_image(iamge, cloud_mask, added_cloud_mask):\n",
    "    channel_count = 11\n",
    "    channel_list = [3, 2, 1]\n",
    "    channels, h, w = iamge.shape\n",
    "    image = iamge[0:channel_count, :, :]\n",
    "\n",
    "    image = np.transpose(image, (1, 2, 0))\n",
    "    pseudo_color_image = image[:, :, channel_list]\n",
    "    # nan_mask = np.isnan(pseudo_color_image).any(axis=-1)  # 掩码，标记 NaN 的位置\n",
    "\n",
    "    background_cloud_image = np.full((h, w, 3), 0, dtype=np.uint8)  # ALL black\n",
    "    background_cloud_image[cloud_mask.squeeze(0) == 1] = [\n",
    "        255,\n",
    "        255,\n",
    "        255,\n",
    "    ]  # Eecept mask white\n",
    "\n",
    "    background_cloud_image[added_cloud_mask.squeeze(0) == 0] = [\n",
    "        255,\n",
    "        0,\n",
    "        0,\n",
    "    ]\n",
    "    return pseudo_color_image, background_cloud_image\n",
    "\n",
    "\n",
    "def test_image_transformation(dataset, idx):\n",
    "\n",
    "    masked_kspace, original_image, cloud_mask, added_cloud_masks_concat, cls_targets, _ = dataset[\n",
    "        idx\n",
    "    ]\n",
    "    time_series_images_original, background_cloud_images_original = (\n",
    "        create_time_series_pseudo_color_image(\n",
    "            original_image, cloud_mask, added_cloud_masks_concat\n",
    "        )\n",
    "    )\n",
    "    time_series_images_masked, background_cloud_images_masked = (\n",
    "        create_time_series_pseudo_color_image(\n",
    "            masked_kspace, cloud_mask, added_cloud_masks_concat\n",
    "        )\n",
    "    )\n",
    "    all_images = [\n",
    "        time_series_images_original,\n",
    "        background_cloud_images_original,\n",
    "        time_series_images_masked,\n",
    "        background_cloud_images_masked,\n",
    "    ]\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(all_images), figsize=(10, 10))\n",
    "    dict_name = {\n",
    "        0: \"Original\",\n",
    "        1: \"Mask\",\n",
    "        2: \"Masked\",\n",
    "        3: \"Masked+Cloud\",\n",
    "    }\n",
    "    for i, image in enumerate(all_images):\n",
    "        axes[i].imshow(image)\n",
    "        axes[i].axis(\"on\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "idx = 20\n",
    "test_image_transformation(train_dataset, idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#Params: 1569408\n",
      "VisionTransformerWithSARCLS(\n",
      "  (feature_extractor): ReconNet(\n",
      "    (net): VisionTransformer(\n",
      "      (patch_embed): PatchEmbed(\n",
      "        (proj): Conv2d(11, 88, kernel_size=(5, 5), stride=(5, 5))\n",
      "      )\n",
      "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
      "      (blocks): ModuleList(\n",
      "        (0-5): 6 x Block(\n",
      "          (norm1): LayerNorm((88,), eps=1e-05, elementwise_affine=True)\n",
      "          (attn): MHSA(\n",
      "            (attend): Softmax(dim=-1)\n",
      "            (to_qkv): Linear(in_features=88, out_features=1056, bias=False)\n",
      "            (to_out): Sequential(\n",
      "              (0): Linear(in_features=352, out_features=88, bias=True)\n",
      "              (1): Dropout(p=0.0, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (drop_path): Identity()\n",
      "          (norm2): LayerNorm((88,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Mlp(\n",
      "            (fc1): Linear(in_features=88, out_features=352, bias=True)\n",
      "            (act): GELU(approximate='none')\n",
      "            (fc2): Linear(in_features=352, out_features=88, bias=True)\n",
      "            (drop): Dropout(p=0.0, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((88,), eps=1e-05, elementwise_affine=True)\n",
      "      (head): Linear(in_features=88, out_features=275, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (classifier): Classifier(\n",
      "    (conv1): Conv2d(11, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv4): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout): Dropout(p=0.5, inplace=False)\n",
      "    (conv5): Conv2d(512, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1038266/3590284791.py:88: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch.nn as nn\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Initialising Models\n",
    "# --------------------------------------------------------------------\n",
    "class VisionTransformerWithSARCLS(nn.Module):\n",
    "    def __init__(self, vision_transformer, classifier):\n",
    "        super(VisionTransformerWithSARCLS, self).__init__()\n",
    "        self.feature_extractor = vision_transformer.to(device)\n",
    "        self.classifier = classifier.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.feature_extractor(x)\n",
    "        output = self.classifier(features)\n",
    "        return output\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Initialising Models\n",
    "# --------------------------------------------------------------------\n",
    "masked_kspace, _, _, _, _,_ = train_dataset[0]\n",
    "C, H, W = masked_kspace.shape\n",
    "\n",
    "channel_count = 11\n",
    "avrg_img_size = 180  # Don't change this\n",
    "patch_size = 5  # 10\n",
    "depth = 6\n",
    "num_heads = 8\n",
    "embed_dim = 11  # 64\n",
    "time_span = C // channel_count\n",
    "in_chans = C\n",
    "epoch_max = 100\n",
    "lr = 1e-4\n",
    "is_SGD = False\n",
    "batch_size = 60\n",
    "seed = 42\n",
    "num_classes = 2\n",
    "\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f\"/home/snt/projects_lujun/vitCausalSeries/data/training/logs/my_experiment_{current_time}_reconstruction_single_vit\"\n",
    "path = log_dir\n",
    "\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "trainloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    # num_workers=2,\n",
    "    pin_memory=True,\n",
    "    generator=torch.Generator().manual_seed(seed),\n",
    ")\n",
    "valloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    # num_workers=1,\n",
    "    pin_memory=True,\n",
    "    generator=torch.Generator().manual_seed(seed),\n",
    ")\n",
    "\n",
    "\n",
    "net = VisionTransformer(\n",
    "    avrg_img_size=avrg_img_size,\n",
    "    patch_size=patch_size,\n",
    "    in_chans=in_chans,\n",
    "    embed_dim=embed_dim,\n",
    "    depth=depth,\n",
    "    num_heads=num_heads,\n",
    "    is_LSA=False,  # ---------------Parameter for adding LSA component\n",
    "    is_SPT=False,  # ---------------Parameter for adding SPT component\n",
    "    rotary_position_emb=False,  # ---------------Parameter for adding ROPE component\n",
    "    use_pos_embed=True,\n",
    ")\n",
    "\n",
    "\n",
    "transformer_net = ReconNet(net).to(device)\n",
    "classifier = Classifier(in_channels=channel_count, cls_class=2).to(device)\n",
    "model = VisionTransformerWithSARCLS(transformer_net, classifier)\n",
    "\n",
    "checkpoint_base_folder = \"/home/snt/projects_lujun/vitCausalSeries/data/training/logs\"\n",
    "training_model_folder = \"my_experiment_20250109-121958_reconstruction_single_vit\"\n",
    "checkpoint_path = f\"{checkpoint_base_folder}/{training_model_folder}/checkpoint_60.pth\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.feature_extractor.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "\n",
    "\n",
    "## Set biases to zero\n",
    "for name, param in model.named_parameters():\n",
    "    if name.endswith(\".bias\"):\n",
    "        torch.nn.init.constant_(param, 0)\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Freeze the feature extractor\n",
    "for param in model.feature_extractor.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"#Params:\", sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "print(model)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Function to Save model\n",
    "# --------------------------------------------------------------------\n",
    "def save_model(path, model, train_hist, optimizer, scheduler=None, epoch=None):\n",
    "    if epoch is None:\n",
    "        epoch = 0\n",
    "    else:\n",
    "        epoch = epoch + 1\n",
    "    if scheduler:\n",
    "        checkpoint = {\n",
    "            \"model\": VisionTransformerWithSARCLS(model.feature_extractor, model.classifier),\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "        }\n",
    "    else:\n",
    "        checkpoint = {\n",
    "            \"model\": VisionTransformerWithSARCLS(model.feature_extractor, model.classifier),\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        }\n",
    "\n",
    "    torch.save(train_hist, path + f\"/train_hist_{epoch}.pt\")\n",
    "    torch.save(checkpoint, path + f\"/checkpoint_{epoch}.pth\")\n",
    "\n",
    "criterion_cls = CrossEntropyWithNaNMask(num_classes=num_classes).to(device)\n",
    "\n",
    "\n",
    "\"\"\"Choose optimizer\"\"\"\n",
    "\n",
    "# criterion = SSIMLoss_V1(window_size=11, channel=1).to(device)\n",
    "# criterion = ms_ssim().to(device)\n",
    "# criterion = piq.ssim().to(device)\n",
    "criterion = MultiScaleLoss(scales=[1, 0.5, 0.25], weight_mse=1.0, weight_sam=1.0).to(\n",
    "    device\n",
    ")\n",
    "\n",
    "\n",
    "if is_SGD:\n",
    "    optimizerG = optim.SGD(\n",
    "        model.classifier.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=0.9,\n",
    "    )\n",
    "    cycle_momentum = True\n",
    "    base_momentum = (0.85,)\n",
    "    max_momentum = (0.95,)\n",
    "\n",
    "else:\n",
    "    optimizerG = optim.Adam(model.classifier.parameters(), lr=0.0)\n",
    "    cycle_momentum = False\n",
    "    base_momentum = 0.0\n",
    "    max_momentum = 0.0\n",
    "\n",
    "\n",
    "train_hist = []\n",
    "\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizerG,\n",
    "    max_lr=lr,\n",
    "    total_steps=epoch_max,\n",
    "    pct_start=0.1,\n",
    "    anneal_strategy=\"cos\",\n",
    "    cycle_momentum=cycle_momentum,\n",
    "    base_momentum=base_momentum,\n",
    "    max_momentum=max_momentum,\n",
    "    div_factor=0.1 * epoch_max,\n",
    "    final_div_factor=9,\n",
    ")\n",
    "\n",
    "\n",
    "def check_nan_in_tensors(*tensors):\n",
    "    for idx, tensor in enumerate(tensors):\n",
    "        if torch.isnan(tensor).any():\n",
    "            print(f\"Tensor {idx} contains NaN values.\")\n",
    "            return True\n",
    "    print(\"No NaN values found in the provided tensors.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train loss.: 8.5560e-01\n",
      "Epoch 1 - Validation Loss: 3.3831e+01\n",
      "Epoch 1 - Overall Accuracy: 0.7991\n",
      "Epoch 1 - Classification F1 Score: 0.7443\n",
      "Epoch 2, Train loss.: 7.8523e-01\n",
      "Epoch 2 - Validation Loss: 3.3650e+01\n",
      "Epoch 2 - Overall Accuracy: 0.8001\n",
      "Epoch 2 - Classification F1 Score: 0.7447\n",
      "Epoch 3, Train loss.: 7.8312e-01\n",
      "Epoch 3 - Validation Loss: 3.3581e+01\n",
      "Epoch 3 - Overall Accuracy: 0.7984\n",
      "Epoch 3 - Classification F1 Score: 0.7494\n",
      "Epoch 4, Train loss.: 7.8238e-01\n",
      "Epoch 4 - Validation Loss: 3.3541e+01\n",
      "Epoch 4 - Overall Accuracy: 0.8008\n",
      "Epoch 4 - Classification F1 Score: 0.7464\n",
      "Epoch 5, Train loss.: 7.8155e-01\n",
      "Epoch 5 - Validation Loss: 3.3506e+01\n",
      "Epoch 5 - Overall Accuracy: 0.8014\n",
      "Epoch 5 - Classification F1 Score: 0.7470\n",
      "Model saved at epoch 5\n",
      "Epoch 6, Train loss.: 7.8115e-01\n",
      "Epoch 6 - Validation Loss: 3.3574e+01\n",
      "Epoch 6 - Overall Accuracy: 0.7996\n",
      "Epoch 6 - Classification F1 Score: 0.7470\n",
      "Epoch 7, Train loss.: 7.8126e-01\n",
      "Epoch 7 - Validation Loss: 3.3537e+01\n",
      "Epoch 7 - Overall Accuracy: 0.8000\n",
      "Epoch 7 - Classification F1 Score: 0.7444\n",
      "Epoch 8, Train loss.: 7.8070e-01\n",
      "Epoch 8 - Validation Loss: 3.3476e+01\n",
      "Epoch 8 - Overall Accuracy: 0.8014\n",
      "Epoch 8 - Classification F1 Score: 0.7471\n",
      "Epoch 9, Train loss.: 7.8083e-01\n",
      "Epoch 9 - Validation Loss: 3.3466e+01\n",
      "Epoch 9 - Overall Accuracy: 0.7989\n",
      "Epoch 9 - Classification F1 Score: 0.7497\n",
      "Epoch 10, Train loss.: 7.8054e-01\n",
      "Epoch 10 - Validation Loss: 3.3488e+01\n",
      "Epoch 10 - Overall Accuracy: 0.8016\n",
      "Epoch 10 - Classification F1 Score: 0.7464\n",
      "Model saved at epoch 10\n",
      "Epoch 11, Train loss.: 7.8100e-01\n",
      "Epoch 11 - Validation Loss: 3.3424e+01\n",
      "Epoch 11 - Overall Accuracy: 0.8019\n",
      "Epoch 11 - Classification F1 Score: 0.7485\n",
      "Epoch 12, Train loss.: 7.7885e-01\n",
      "Epoch 12 - Validation Loss: 3.3442e+01\n",
      "Epoch 12 - Overall Accuracy: 0.8027\n",
      "Epoch 12 - Classification F1 Score: 0.7461\n",
      "Epoch 13, Train loss.: 7.7958e-01\n",
      "Epoch 13 - Validation Loss: 3.3421e+01\n",
      "Epoch 13 - Overall Accuracy: 0.8027\n",
      "Epoch 13 - Classification F1 Score: 0.7477\n",
      "Epoch 14, Train loss.: 7.7827e-01\n",
      "Epoch 14 - Validation Loss: 3.3485e+01\n",
      "Epoch 14 - Overall Accuracy: 0.7981\n",
      "Epoch 14 - Classification F1 Score: 0.7501\n",
      "Epoch 15, Train loss.: 7.7756e-01\n",
      "Epoch 15 - Validation Loss: 3.3467e+01\n",
      "Epoch 15 - Overall Accuracy: 0.7996\n",
      "Epoch 15 - Classification F1 Score: 0.7502\n",
      "Model saved at epoch 15\n",
      "Epoch 16, Train loss.: 7.7793e-01\n",
      "Epoch 16 - Validation Loss: 3.3445e+01\n",
      "Epoch 16 - Overall Accuracy: 0.8026\n",
      "Epoch 16 - Classification F1 Score: 0.7468\n",
      "Epoch 17, Train loss.: 7.7755e-01\n",
      "Epoch 17 - Validation Loss: 3.3465e+01\n",
      "Epoch 17 - Overall Accuracy: 0.7994\n",
      "Epoch 17 - Classification F1 Score: 0.7489\n",
      "Epoch 18, Train loss.: 7.7653e-01\n",
      "Epoch 18 - Validation Loss: 3.3513e+01\n",
      "Epoch 18 - Overall Accuracy: 0.7986\n",
      "Epoch 18 - Classification F1 Score: 0.7480\n",
      "Epoch 19, Train loss.: 7.7725e-01\n",
      "Epoch 19 - Validation Loss: 3.3447e+01\n",
      "Epoch 19 - Overall Accuracy: 0.8016\n",
      "Epoch 19 - Classification F1 Score: 0.7497\n",
      "Epoch 20, Train loss.: 7.7563e-01\n",
      "Epoch 20 - Validation Loss: 3.3396e+01\n",
      "Epoch 20 - Overall Accuracy: 0.8020\n",
      "Epoch 20 - Classification F1 Score: 0.7499\n",
      "Model saved at epoch 20\n",
      "Epoch 21, Train loss.: 7.7522e-01\n",
      "Epoch 21 - Validation Loss: 3.3386e+01\n",
      "Epoch 21 - Overall Accuracy: 0.7998\n",
      "Epoch 21 - Classification F1 Score: 0.7502\n",
      "Epoch 22, Train loss.: 7.7550e-01\n",
      "Epoch 22 - Validation Loss: 3.4060e+01\n",
      "Epoch 22 - Overall Accuracy: 0.8008\n",
      "Epoch 22 - Classification F1 Score: 0.7477\n",
      "Epoch 23, Train loss.: 7.7474e-01\n",
      "Epoch 23 - Validation Loss: 3.3455e+01\n",
      "Epoch 23 - Overall Accuracy: 0.7994\n",
      "Epoch 23 - Classification F1 Score: 0.7484\n",
      "Epoch 24, Train loss.: 7.7403e-01\n",
      "Epoch 24 - Validation Loss: 3.3434e+01\n",
      "Epoch 24 - Overall Accuracy: 0.8012\n",
      "Epoch 24 - Classification F1 Score: 0.7501\n",
      "Epoch 25, Train loss.: 7.7402e-01\n",
      "Epoch 25 - Validation Loss: 3.3532e+01\n",
      "Epoch 25 - Overall Accuracy: 0.7979\n",
      "Epoch 25 - Classification F1 Score: 0.7514\n",
      "Model saved at epoch 25\n",
      "Epoch 26, Train loss.: 7.7299e-01\n",
      "Epoch 26 - Validation Loss: 3.3397e+01\n",
      "Epoch 26 - Overall Accuracy: 0.7990\n",
      "Epoch 26 - Classification F1 Score: 0.7515\n",
      "Epoch 27, Train loss.: 7.7299e-01\n",
      "Epoch 27 - Validation Loss: 3.3477e+01\n",
      "Epoch 27 - Overall Accuracy: 0.7986\n",
      "Epoch 27 - Classification F1 Score: 0.7534\n",
      "Epoch 28, Train loss.: 7.7325e-01\n",
      "Epoch 28 - Validation Loss: 3.3376e+01\n",
      "Epoch 28 - Overall Accuracy: 0.7993\n",
      "Epoch 28 - Classification F1 Score: 0.7531\n",
      "Epoch 29, Train loss.: 7.7245e-01\n",
      "Epoch 29 - Validation Loss: 3.3563e+01\n",
      "Epoch 29 - Overall Accuracy: 0.7957\n",
      "Epoch 29 - Classification F1 Score: 0.7530\n",
      "Epoch 30, Train loss.: 7.7184e-01\n",
      "Epoch 30 - Validation Loss: 3.3374e+01\n",
      "Epoch 30 - Overall Accuracy: 0.8016\n",
      "Epoch 30 - Classification F1 Score: 0.7512\n",
      "Model saved at epoch 30\n",
      "Epoch 31, Train loss.: 7.7174e-01\n",
      "Epoch 31 - Validation Loss: 3.3446e+01\n",
      "Epoch 31 - Overall Accuracy: 0.7992\n",
      "Epoch 31 - Classification F1 Score: 0.7536\n",
      "Epoch 32, Train loss.: 7.7038e-01\n",
      "Epoch 32 - Validation Loss: 3.3481e+01\n",
      "Epoch 32 - Overall Accuracy: 0.7986\n",
      "Epoch 32 - Classification F1 Score: 0.7536\n",
      "Epoch 33, Train loss.: 7.7034e-01\n",
      "Epoch 33 - Validation Loss: 3.3465e+01\n",
      "Epoch 33 - Overall Accuracy: 0.7962\n",
      "Epoch 33 - Classification F1 Score: 0.7537\n",
      "Epoch 34, Train loss.: 7.6925e-01\n",
      "Epoch 34 - Validation Loss: 3.3471e+01\n",
      "Epoch 34 - Overall Accuracy: 0.7980\n",
      "Epoch 34 - Classification F1 Score: 0.7519\n",
      "Epoch 35, Train loss.: 7.6973e-01\n",
      "Epoch 35 - Validation Loss: 3.3404e+01\n",
      "Epoch 35 - Overall Accuracy: 0.7982\n",
      "Epoch 35 - Classification F1 Score: 0.7547\n",
      "Model saved at epoch 35\n",
      "Epoch 36, Train loss.: 7.6854e-01\n",
      "Epoch 36 - Validation Loss: 3.3612e+01\n",
      "Epoch 36 - Overall Accuracy: 0.7947\n",
      "Epoch 36 - Classification F1 Score: 0.7529\n",
      "Epoch 37, Train loss.: 7.6856e-01\n",
      "Epoch 37 - Validation Loss: 3.3482e+01\n",
      "Epoch 37 - Overall Accuracy: 0.7973\n",
      "Epoch 37 - Classification F1 Score: 0.7518\n",
      "Epoch 38, Train loss.: 7.6844e-01\n",
      "Epoch 38 - Validation Loss: 3.3509e+01\n",
      "Epoch 38 - Overall Accuracy: 0.7963\n",
      "Epoch 38 - Classification F1 Score: 0.7530\n",
      "Epoch 39, Train loss.: 7.6733e-01\n",
      "Epoch 39 - Validation Loss: 3.3486e+01\n",
      "Epoch 39 - Overall Accuracy: 0.7982\n",
      "Epoch 39 - Classification F1 Score: 0.7527\n",
      "Epoch 40, Train loss.: 7.6707e-01\n",
      "Epoch 40 - Validation Loss: 3.3573e+01\n",
      "Epoch 40 - Overall Accuracy: 0.7955\n",
      "Epoch 40 - Classification F1 Score: 0.7503\n",
      "Model saved at epoch 40\n",
      "Epoch 41, Train loss.: 7.6738e-01\n",
      "Epoch 41 - Validation Loss: 3.3494e+01\n",
      "Epoch 41 - Overall Accuracy: 0.7959\n",
      "Epoch 41 - Classification F1 Score: 0.7526\n",
      "Epoch 42, Train loss.: 7.6617e-01\n",
      "Epoch 42 - Validation Loss: 3.3671e+01\n",
      "Epoch 42 - Overall Accuracy: 0.7929\n",
      "Epoch 42 - Classification F1 Score: 0.7499\n",
      "Epoch 43, Train loss.: 7.6516e-01\n",
      "Epoch 43 - Validation Loss: 3.3537e+01\n",
      "Epoch 43 - Overall Accuracy: 0.7967\n",
      "Epoch 43 - Classification F1 Score: 0.7533\n",
      "Epoch 44, Train loss.: 7.6557e-01\n",
      "Epoch 44 - Validation Loss: 3.3571e+01\n",
      "Epoch 44 - Overall Accuracy: 0.7950\n",
      "Epoch 44 - Classification F1 Score: 0.7527\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     10\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(trainloader):\n\u001b[1;32m     13\u001b[0m     inputs, targets, cloud_mask, added_cloud_mask, cls_targets, _ \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     14\u001b[0m     channel_each \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m time_span\n",
      "File \u001b[0;32m~/miniconda3/envs/causal_vit_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/causal_vit_env/lib/python3.10/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/causal_vit_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/causal_vit_env/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[19], line 81\u001b[0m, in \u001b[0;36mImagenetDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m     80\u001b[0m     channel_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m\n\u001b[0;32m---> 81\u001b[0m     tiff \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_tiff\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfiles\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     82\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfiles[idx]\n\u001b[1;32m     83\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_path)\n",
      "Cell \u001b[0;32mIn[19], line 39\u001b[0m, in \u001b[0;36mImagenetDataset.read_tiff\u001b[0;34m(self, file_path)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, band_count \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     38\u001b[0m     band \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mGetRasterBand(i)\n\u001b[0;32m---> 39\u001b[0m     band_name \u001b[38;5;241m=\u001b[39m \u001b[43mband\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGetDescription\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m     band_data \u001b[38;5;241m=\u001b[39m band\u001b[38;5;241m.\u001b[39mReadAsArray()\n\u001b[1;32m     41\u001b[0m     bands_data\u001b[38;5;241m.\u001b[39mappend(band_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/causal_vit_env/lib/python3.10/site-packages/osgeo/gdal.py:1921\u001b[0m, in \u001b[0;36mMajorObject.GetDescription\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mGetDescription\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"GetDescription(MajorObject self) -> char const *\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_gdal\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMajorObject_GetDescription\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Start to train the model\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "# Initialize TensorBoard writer\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "for epoch in range(0, epoch_max):  # loop over the dataset multiple times\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "        inputs, targets, cloud_mask, added_cloud_mask, cls_targets, _ = data\n",
    "        channel_each = inputs.shape[1] // time_span\n",
    "        last_time_start = channel_each * (time_span - 1)\n",
    "        last_time_end = channel_each * time_span\n",
    "        optimizerG.zero_grad()\n",
    "        images_outputs = model.feature_extractor(inputs.to(device))\n",
    "        cls_outputs = model.classifier(images_outputs.to(device))\n",
    "        valid_pixels_total = cloud_mask.sum()\n",
    "        if valid_pixels_total > 0:\n",
    "            loss = criterion_cls(\n",
    "                cls_outputs.to(device),\n",
    "                cls_targets.to(device),\n",
    "            )\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=device, requires_grad=True)\n",
    "            \n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            model.parameters(), max_norm=1, norm_type=1, error_if_nonfinite=True\n",
    "        )\n",
    "        optimizerG.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    writer.add_scalar(\"Loss/Train\", train_loss, epoch)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, cloud_mask, added_cloud_mask, cls_targets, _ in valloader:\n",
    "            images_outputs = model.feature_extractor(inputs.to(device))\n",
    "            cls_outputs = model.classifier(images_outputs.to(device))\n",
    "            # Apply cloud mask to outputs and targets\n",
    "\n",
    "            # Compute validation loss\n",
    "            valid_pixels_total = cloud_mask.sum()\n",
    "            if valid_pixels_total > 0:\n",
    "                loss = criterion_cls(\n",
    "                    cls_outputs.to(device),\n",
    "                    cls_targets.to(device),\n",
    "                )\n",
    "            else:\n",
    "                loss = torch.tensor(0.0, device=device, requires_grad=False)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and targets for additional metrics\n",
    "            preds = (\n",
    "                cls_outputs.argmax(dim=1).cpu().numpy()\n",
    "            )  # Assuming class indices are returned as outputs\n",
    "            targets_cls = cls_targets.cpu().numpy()\n",
    "\n",
    "            all_preds.extend(preds)\n",
    "            all_targets.extend(targets_cls)\n",
    "\n",
    "        # Compute additional metrics\n",
    "        OAs = []\n",
    "        F1s = []\n",
    "        for i in range(len(all_preds)):\n",
    "            overall_accuracy = calculate_accuracy_excluding_label(\n",
    "                all_targets[i], all_preds[i], num_classes\n",
    "            )\n",
    "            f1 = calculate_f1_score_excluding_label(\n",
    "                all_targets[i], all_preds[i], num_classes\n",
    "            )\n",
    "            OAs.append(overall_accuracy)\n",
    "            F1s.append(f1)\n",
    "        average_accuracy = np.mean(OAs)\n",
    "        average_f1 = np.mean(F1s)\n",
    "\n",
    "    # Log validation loss and metrics to TensorBoard\n",
    "    writer.add_scalar(\"Loss/Validation\", val_loss, epoch)\n",
    "    writer.add_scalar(\"Accuracy/Validation\", average_accuracy, epoch)\n",
    "    writer.add_scalar(\"F1_Score/Validation\", average_f1, epoch)\n",
    "\n",
    "    if not is_SGD:\n",
    "        scheduler.step()\n",
    "    train_hist.append(train_loss / len(trainloader))\n",
    "    # save_model(path, model, train_hist, optimizerG, scheduler=scheduler)\n",
    "    print(\"Epoch {}, Train loss.: {:0.4e}\".format(epoch + 1, train_hist[-1]))\n",
    "    print(f\"Epoch {epoch + 1} - Validation Loss: {val_loss:.4e}\")\n",
    "    print(f\"Epoch {epoch + 1} - Overall Accuracy: {average_accuracy:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1} - Classification F1 Score: {average_f1:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        save_model(\n",
    "            path, model, train_hist, optimizerG, scheduler=scheduler, epoch=epoch\n",
    "        )\n",
    "        print(f\"Model saved at epoch {epoch + 1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImagenetDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        isval=False,\n",
    "        add_cloud_mask=True,\n",
    "        add_kspace_mask=False,\n",
    "        val_path=\"/home/snt/projects_lujun/Image-Reconstruction-by-Vision-Transformer/output_all_times/val\",\n",
    "        train_path=\"/home/snt/projects_lujun/Image-Reconstruction-by-Vision-Transformer/output_all_times/train\",\n",
    "    ):\n",
    "\n",
    "        self.add_cloud_mask = add_cloud_mask\n",
    "        self.add_kspace_mask = add_kspace_mask\n",
    "        self.val_path = val_path\n",
    "        self.train_path = train_path\n",
    "        if isval:\n",
    "            ## combine paths of each imagenet validation image into a single list\n",
    "            self.files = []\n",
    "            pattern = \"*.tif\"\n",
    "            for dir, _, _ in os.walk(self.val_path):\n",
    "                self.files.extend(glob(os.path.join(dir, pattern)))\n",
    "        else:\n",
    "            ## combine paths of each imagenet training image into a single list\n",
    "            self.files = []  # get path of each imagenet images\n",
    "            pattern = \"*.tif\"\n",
    "            for dir, _, _ in os.walk(self.train_path):\n",
    "                self.files.extend(glob(os.path.join(dir, pattern)))\n",
    "\n",
    "        self.transform = transform_tensor_image\n",
    "        self.factors = [2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "    def read_tiff(self, file_path):\n",
    "        dataset = gdal.Open(file_path)\n",
    "        if not dataset:\n",
    "            raise FileNotFoundError(f\"Unable to open the file: {file_path}\")\n",
    "        band_count = dataset.RasterCount\n",
    "        bands_data = []\n",
    "        for i in range(1, band_count + 1):\n",
    "            band = dataset.GetRasterBand(i)\n",
    "            band_name = band.GetDescription()\n",
    "            band_data = band.ReadAsArray()\n",
    "            bands_data.append(band_data)\n",
    "\n",
    "        image = np.stack(bands_data, axis=0)\n",
    "        return image\n",
    "\n",
    "    def get_mask_func(\n",
    "        self,\n",
    "        samp_style,\n",
    "        factor,\n",
    "    ):\n",
    "        center_fractions = 0.08 * 4 / factor\n",
    "        if samp_style == \"random\":\n",
    "            mask_func = subsample.RandomMaskFunc(\n",
    "                center_fractions=[center_fractions],\n",
    "                accelerations=[factor],\n",
    "            )\n",
    "        elif samp_style == \"equidist\":\n",
    "            mask_func = subsample.EquispacedMaskFunc(\n",
    "                center_fractions=[center_fractions],\n",
    "                accelerations=[factor],\n",
    "            )\n",
    "        return mask_func\n",
    "\n",
    "    def add_gaussian_noise(self, x):\n",
    "        ch, row, col = x.shape\n",
    "        mean = 0\n",
    "        var = 0.05\n",
    "        sigma = var**0.5\n",
    "        gauss = np.random.normal(mean, sigma, (ch, row, col))\n",
    "        gauss = gauss.reshape(ch, row, col)\n",
    "        noisy = x + gauss\n",
    "        return noisy.float()\n",
    "\n",
    "    def __len__(\n",
    "        self,\n",
    "    ):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        channel_count = 11\n",
    "        tiff = torch.from_numpy(self.read_tiff(self.files[idx]))\n",
    "        file_path = self.files[idx]\n",
    "        file_name = os.path.basename(file_path)\n",
    "        # tiff = self.transform(tiff, target_size=64 * 3, crop_size=60 * 3)\n",
    "        channels, height, width = tiff.shape\n",
    "        time_span = int((channels - 1) / channel_count)\n",
    "        image = tiff[0:channel_count, :, :]\n",
    "\n",
    "        y = image\n",
    "        cloud_mask = torch.isnan(y).any(dim=0)\n",
    "        cloud_mask = (~cloud_mask).int()\n",
    "        y[:, cloud_mask == 0] = 0\n",
    "        original_spaces_transformed = y.clone()\n",
    "        masked_kspace = y.clone()\n",
    "\n",
    "        if random.uniform(0, 1) < 0.5:\n",
    "            y = torch.rot90(y, 1, [-2, -1])\n",
    "\n",
    "        if random.uniform(0, 1) < 0.5:\n",
    "            samp_style = \"random\"\n",
    "        else:\n",
    "            samp_style = \"equidist\"\n",
    "\n",
    "        # factor = random.choice(self.factors)\n",
    "        mask_func = self.get_mask_func(samp_style, factor=2.0)\n",
    "\n",
    "        if self.add_kspace_mask:  # add kspace mask\n",
    "            masked_kspace, _ = transforms.apply_mask(masked_kspace, mask_func)\n",
    "        if self.add_cloud_mask:  # add cloud mask\n",
    "            masked_kspace, added_cloud_mask = RealisticCloudMaskFunc(\n",
    "                    masked_kspace, clouds_attributes={\"cloud_count\": 0}\n",
    "                )\n",
    "            added_cloud_mask = added_cloud_mask.unsqueeze(0)\n",
    "        else:\n",
    "            added_cloud_mask = torch.ones_like(cloud_mask.unsqueeze(0))\n",
    "\n",
    "        cls_targets = np.squeeze(tiff[-1])\n",
    "\n",
    "        unique_classes = torch.unique(cls_targets[~torch.isnan(cls_targets)])\n",
    "        num_class = len(unique_classes)\n",
    "        class_mapping = {value.item(): idx for idx, value in enumerate(unique_classes)}\n",
    "        cls_targets = torch.nan_to_num(cls_targets, nan=num_class)\n",
    "        cls_targets = cls_targets.apply_(lambda x: class_mapping.get(x, num_class))\n",
    "        cls_targets = cls_targets.long()\n",
    "\n",
    "        return (\n",
    "            masked_kspace,  # inputs\n",
    "            original_spaces_transformed,  # tragets\n",
    "            cloud_mask,\n",
    "            added_cloud_mask,\n",
    "            cls_targets,\n",
    "            file_name,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1038266/3641123685.py:41: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "if_write = False\n",
    "current_time = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = f\"/home/snt/projects_lujun/vitCausalSeries/data/training/logs/my_experiment_{current_time}\"\n",
    "path = log_dir\n",
    "val_path = \"/home/snt/projects_lujun/vitCausalSeries/data/intermediate_SAR_2021\"\n",
    "checkpoint_base_folder = \"/home/snt/projects_lujun/vitCausalSeries/data/training/logs\"\n",
    "training_model_folder = \"my_experiment_20250116-224713_reconstruction_single_vit\"\n",
    "\n",
    "checkpoint_path = f\"{checkpoint_base_folder}/{training_model_folder}/checkpoint_40.pth\"\n",
    "\n",
    "outputs_folder = f\"{checkpoint_base_folder}/{training_model_folder}/masked_outputs\"\n",
    "targets_folder = f\"{checkpoint_base_folder}/{training_model_folder}/masked_targets\"\n",
    "inputs_folder = f\"{checkpoint_base_folder}/{training_model_folder}/masked_inputs\"\n",
    "cloud_mask_folder = f\"{checkpoint_base_folder}/{training_model_folder}/cloud_mask\"\n",
    "\n",
    "\n",
    "val_dataset = ImagenetDataset(\n",
    "    isval=True,\n",
    "    add_cloud_mask=True,\n",
    "    add_kspace_mask=False,\n",
    "    val_path=val_path,\n",
    ")\n",
    "batch_size = 20\n",
    "valloader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    # num_workers=1,\n",
    "    pin_memory=True,\n",
    "    generator=torch.Generator().manual_seed(seed),\n",
    ")\n",
    "\n",
    "\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "# epoch = checkpoint['epoch']\n",
    "\n",
    "print(\"Checkpoint loaded successfully!\")\n",
    "\n",
    "\n",
    "def write_tiff(output_path, image):\n",
    "    if image.ndim == 2:\n",
    "        image = image[np.newaxis, ...]\n",
    "\n",
    "    bands, height, width = image.shape\n",
    "\n",
    "    driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    dataset = driver.Create(\n",
    "        output_path,\n",
    "        width,\n",
    "        height,\n",
    "        bands,\n",
    "        gdal.GDT_Float32,\n",
    "    )\n",
    "\n",
    "    if dataset is None:\n",
    "        raise RuntimeError(f\"Unable to create TIFF file: {output_path}\")\n",
    "\n",
    "    # Write each band to the dataset\n",
    "    for i in range(bands):\n",
    "        band = dataset.GetRasterBand(i + 1)\n",
    "        band.WriteArray(image[i, :, :])\n",
    "        band.FlushCache()  # Ensure data is written to disk\n",
    "\n",
    "    dataset = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 4.8701e+01\n",
      "Overall Accuracy: 0.5940\n",
      "Classification F1 Score: 0.5136\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "val_loss = 0.0\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, targets, cloud_mask, added_cloud_mask, cls_targets, _ in valloader:\n",
    "        images_outputs = model.feature_extractor(inputs.to(device))\n",
    "        cls_outputs = model.classifier(images_outputs.to(device))\n",
    "        # Apply cloud mask to outputs and targets\n",
    "\n",
    "        # Compute validation loss\n",
    "        valid_pixels_total = cloud_mask.sum()\n",
    "        if valid_pixels_total > 0:\n",
    "            loss = criterion_cls(\n",
    "                cls_outputs.to(device),\n",
    "                cls_targets.to(device),\n",
    "            )\n",
    "        else:\n",
    "            loss = torch.tensor(0.0, device=device, requires_grad=False)\n",
    "\n",
    "        val_loss += loss.item()\n",
    "\n",
    "        # Collect predictions and targets for additional metrics\n",
    "        preds = (\n",
    "            cls_outputs.argmax(dim=1).cpu().numpy()\n",
    "        )  # Assuming class indices are returned as outputs\n",
    "        targets_cls = cls_targets.cpu().numpy()\n",
    "\n",
    "        all_preds.extend(preds)\n",
    "        all_targets.extend(targets_cls)\n",
    "\n",
    "    # Compute additional metrics\n",
    "    OAs = []\n",
    "    F1s = []\n",
    "    for i in range(len(all_preds)):\n",
    "        overall_accuracy = calculate_accuracy_excluding_label(\n",
    "            all_targets[i], all_preds[i], num_classes\n",
    "        )\n",
    "        f1 = calculate_f1_score_excluding_label(\n",
    "            all_targets[i], all_preds[i], num_classes\n",
    "        )\n",
    "        OAs.append(overall_accuracy)\n",
    "        F1s.append(f1)\n",
    "    average_accuracy = np.nanmean(OAs)\n",
    "    average_f1 = np.nanmean(F1s)\n",
    "\n",
    "\n",
    "print(f\"Validation Loss: {val_loss:.4e}\")\n",
    "print(f\"Overall Accuracy: {average_accuracy:.4f}\")\n",
    "print(f\"Classification F1 Score: {average_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "causal_vit_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
